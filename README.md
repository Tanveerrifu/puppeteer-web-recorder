# puppeteer-web-recorder

This project is a **headless web crawler built with Puppeteer** that visits all internal pages of a website, waits for full page load, and saves each page as a **full-page PDF** in an organized folder structure. Useful for **QA engineers**, **web archivists**, and **content reviewers**.

---

## ðŸ“Œ Features

âœ… Automatically crawls all internal links on a website  
âœ… Waits 30 seconds per page to allow full rendering  
âœ… Saves every page as a full-page **PDF snapshot**  
âœ… Organizes output in folders using the current date and URL structure  
âœ… Helps track **UI changes across builds**  

---

## ðŸš€ How It Helps QA Teams

- ðŸŽ¯ **Regression Tracking**: Snapshot every page to compare between builds  
- ðŸ“· **Visual Archiving**: Keep historical UI records  
- ðŸ”Ž **Link & Content Audits**: Ensure all internal links are working and rendered  
- âœ… **No need for manual screenshots**

---

## ðŸ›  Setup

### 1. Clone this repo

git clone https://github.com/your-username/website-pdf-crawler.git
cd website-pdf-crawler
2. Install dependencies

npm install puppeteer fs-extra
3. Configure the starting URL
In web_crawler.js, change:


const startUrl = 'https://example.com'; // <-- your site here

4. Run the crawler
node web_crawler.js

ðŸ—‚ Output Example
screenshots/
  â””â”€â”€ 2025-04-02/
      â”œâ”€â”€ home/
      â”‚   â””â”€â”€ page.pdf
      â”œâ”€â”€ about_us/
      â”‚   â””â”€â”€ page.pdf
      â””â”€â”€ contact/
          â””â”€â”€ page.pdf
ðŸ“Œ License
MIT License

